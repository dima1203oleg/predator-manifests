---
# Source: predator-analytics/charts/opensearch/templates/poddisruptionbudget.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: "predator-opensearch-master-pdb"
  labels:
    helm.sh/chart: opensearch-2.35.1
    app.kubernetes.io/name: opensearch
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "2.19.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: predator-opensearch-master
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: opensearch
      app.kubernetes.io/instance: predator-analytics
---
# Source: predator-analytics/charts/grafana/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    helm.sh/chart: grafana-6.61.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "10.1.5"
    app.kubernetes.io/managed-by: Helm
  name: predator-analytics-grafana
  namespace: default
---
# Source: predator-analytics/charts/prometheus/charts/alertmanager/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: predator-analytics-alertmanager
  labels:
    helm.sh/chart: alertmanager-0.33.1
    app.kubernetes.io/name: alertmanager
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "v0.25.0"
    app.kubernetes.io/managed-by: Helm
  namespace: default
automountServiceAccountToken: true
---
# Source: predator-analytics/charts/prometheus/charts/kube-state-metrics/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    helm.sh/chart: kube-state-metrics-5.10.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "2.9.2"
  name: predator-analytics-kube-state-metrics
  namespace: default
imagePullSecrets:
---
# Source: predator-analytics/charts/prometheus/charts/prometheus-node-exporter/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: predator-analytics-prometheus-node-exporter
  namespace: default
  labels:
    helm.sh/chart: prometheus-node-exporter-4.21.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "1.6.0"
---
# Source: predator-analytics/charts/prometheus/charts/prometheus-pushgateway/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    helm.sh/chart: prometheus-pushgateway-2.4.0
    app.kubernetes.io/name: prometheus-pushgateway
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "v1.6.0"
    app.kubernetes.io/managed-by: Helm
  name: predator-analytics-prometheus-pushgateway
  namespace: default
---
# Source: predator-analytics/charts/prometheus/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: v2.46.0
    helm.sh/chart: prometheus-23.4.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: prometheus
  name: predator-analytics-prometheus-server
  namespace: default
  annotations: {}
---
# Source: predator-analytics/charts/redis/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: true
metadata:
  name: predator-analytics-redis
  namespace: "default"
  labels:
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.0.12
    helm.sh/chart: redis-17.17.1
---
# Source: predator-analytics/charts/grafana/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: predator-analytics-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-6.61.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "10.1.5"
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  admin-user: "YWRtaW4="
  admin-password: "ZDE4TnlxU1ViY1BIenN4RlFESm0wM2VDdE0zRUt3NjFxa3k4U2JmTg=="
  ldap-toml: ""
---
# Source: predator-analytics/charts/postgresql/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: predator-analytics-postgresql
  namespace: "default"
  labels:
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 15.4.0
    helm.sh/chart: postgresql-12.12.10
type: Opaque
data:
  postgres-password: "ZEpPZjJ6TlZKMA=="
  password: "UkoxS2hXTDQ5Ug=="
  # We don't auto-generate LDAP password when it's not provided as we do for other passwords
---
# Source: predator-analytics/charts/redis/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: predator-analytics-redis
  namespace: "default"
  labels:
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.0.12
    helm.sh/chart: redis-17.17.1
type: Opaque
data:
  redis-password: "VHE3TEZlWFRINA=="
---
# Source: predator-analytics/charts/grafana/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: predator-analytics-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-6.61.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "10.1.5"
    app.kubernetes.io/managed-by: Helm
data:
  grafana.ini: |
    [analytics]
    check_for_updates = true
    [grafana_net]
    url = https://grafana.net
    [log]
    mode = console
    [paths]
    data = /var/lib/grafana/
    logs = /var/log/grafana
    plugins = /var/lib/grafana/plugins
    provisioning = /etc/grafana/provisioning
    [server]
    domain = ''
---
# Source: predator-analytics/charts/opensearch/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: predator-opensearch-master-config
  labels:
    helm.sh/chart: opensearch-2.35.1
    app.kubernetes.io/name: opensearch
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "2.19.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: predator-opensearch-master
data:
  opensearch.yml: |
    cluster.name: opensearch-cluster

    # Bind to all interfaces because we don't know what IP address Docker will assign to us.
    network.host: 0.0.0.0

    # Setting network.host to a non-loopback address enables the annoying bootstrap checks. "Single-node" mode disables them again.
    # Implicitly done if ".singleNode" is set to "true".
    # discovery.type: single-node

    # Start OpenSearch Security Demo Configuration
    # WARNING: revise all the lines below before you go into production
    # plugins:
    #   security:
    #     ssl:
    #       transport:
    #         pemcert_filepath: esnode.pem
    #         pemkey_filepath: esnode-key.pem
    #         pemtrustedcas_filepath: root-ca.pem
    #         enforce_hostname_verification: false
    #       http:
    #         enabled: true
    #         pemcert_filepath: esnode.pem
    #         pemkey_filepath: esnode-key.pem
    #         pemtrustedcas_filepath: root-ca.pem
    #     allow_unsafe_democertificates: true
    #     allow_default_init_securityindex: true
    #     authcz:
    #       admin_dn:
    #         - CN=kirk,OU=client,O=client,L=test,C=de
    #     audit.type: internal_opensearch
    #     enable_snapshot_restore_privilege: true
    #     check_snapshot_restore_write_privileges: true
    #     restapi:
    #       roles_enabled: ["all_access", "security_rest_api_access"]
    #     system_indices:
    #       enabled: true
    #       indices:
    #         [
    #           ".opendistro-alerting-config",
    #           ".opendistro-alerting-alert*",
    #           ".opendistro-anomaly-results*",
    #           ".opendistro-anomaly-detector*",
    #           ".opendistro-anomaly-checkpoints",
    #           ".opendistro-anomaly-detection-state",
    #           ".opendistro-reports-*",
    #           ".opendistro-notifications-*",
    #           ".opendistro-notebooks",
    #           ".opendistro-asynchronous-search-response*",
    #         ]
    ######## End OpenSearch Security Demo Configuration ########
---
# Source: predator-analytics/charts/prometheus/charts/alertmanager/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: predator-analytics-alertmanager
  labels:
    helm.sh/chart: alertmanager-0.33.1
    app.kubernetes.io/name: alertmanager
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "v0.25.0"
    app.kubernetes.io/managed-by: Helm
  namespace: default
data:
  alertmanager.yml: |
    global: {}
    receivers:
    - name: default-receiver
    route:
      group_interval: 5m
      group_wait: 10s
      receiver: default-receiver
      repeat_interval: 3h
    templates:
    - /etc/alertmanager/*.tmpl
---
# Source: predator-analytics/charts/prometheus/templates/cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: v2.46.0
    helm.sh/chart: prometheus-23.4.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: prometheus
  name: predator-analytics-prometheus-server
  namespace: default
data:
  allow-snippet-annotations: "false"
  alerting_rules.yml: |
    {}
  alerts: |
    {}
  prometheus.yml: |
    global:
      evaluation_interval: 1m
      scrape_interval: 1m
      scrape_timeout: 10s
    rule_files:
    - /etc/config/recording_rules.yml
    - /etc/config/alerting_rules.yml
    - /etc/config/rules
    - /etc/config/alerts
    scrape_configs:
    - job_name: prometheus
      static_configs:
      - targets:
        - localhost:9090
    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      job_name: kubernetes-apiservers
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: default;kubernetes;https
        source_labels:
        - __meta_kubernetes_namespace
        - __meta_kubernetes_service_name
        - __meta_kubernetes_endpoint_port_name
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      job_name: kubernetes-nodes
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - replacement: kubernetes.default.svc:443
        target_label: __address__
      - regex: (.+)
        replacement: /api/v1/nodes/$1/proxy/metrics
        source_labels:
        - __meta_kubernetes_node_name
        target_label: __metrics_path__
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      job_name: kubernetes-nodes-cadvisor
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - replacement: kubernetes.default.svc:443
        target_label: __address__
      - regex: (.+)
        replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor
        source_labels:
        - __meta_kubernetes_node_name
        target_label: __metrics_path__
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - honor_labels: true
      job_name: kubernetes-service-endpoints
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scrape
      - action: drop
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scrape_slow
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: (.+?)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_service_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)
        replacement: __param_$1
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_service_name
        target_label: service
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: node
    - honor_labels: true
      job_name: kubernetes-service-endpoints-slow
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scrape_slow
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: (.+?)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_service_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)
        replacement: __param_$1
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_service_name
        target_label: service
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: node
      scrape_interval: 5m
      scrape_timeout: 30s
    - honor_labels: true
      job_name: prometheus-pushgateway
      kubernetes_sd_configs:
      - role: service
      relabel_configs:
      - action: keep
        regex: pushgateway
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_probe
    - honor_labels: true
      job_name: kubernetes-services
      kubernetes_sd_configs:
      - role: service
      metrics_path: /probe
      params:
        module:
        - http_2xx
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_probe
      - source_labels:
        - __address__
        target_label: __param_target
      - replacement: blackbox
        target_label: __address__
      - source_labels:
        - __param_target
        target_label: instance
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - source_labels:
        - __meta_kubernetes_service_name
        target_label: service
    - honor_labels: true
      job_name: kubernetes-pods
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scrape
      - action: drop
        regex: true
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scrape_slow
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: (\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})
        replacement: '[$2]:$1'
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_port
        - __meta_kubernetes_pod_ip
        target_label: __address__
      - action: replace
        regex: (\d+);((([0-9]+?)(\.|$)){4})
        replacement: $2:$1
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_port
        - __meta_kubernetes_pod_ip
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)
        replacement: __param_$1
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_name
        target_label: pod
      - action: drop
        regex: Pending|Succeeded|Failed|Completed
        source_labels:
        - __meta_kubernetes_pod_phase
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: node
    - honor_labels: true
      job_name: kubernetes-pods-slow
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scrape_slow
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: (\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})
        replacement: '[$2]:$1'
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_port
        - __meta_kubernetes_pod_ip
        target_label: __address__
      - action: replace
        regex: (\d+);((([0-9]+?)(\.|$)){4})
        replacement: $2:$1
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_port
        - __meta_kubernetes_pod_ip
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)
        replacement: __param_$1
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_name
        target_label: pod
      - action: drop
        regex: Pending|Succeeded|Failed|Completed
        source_labels:
        - __meta_kubernetes_pod_phase
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: node
      scrape_interval: 5m
      scrape_timeout: 30s
    alerting:
      alertmanagers:
      - kubernetes_sd_configs:
          - role: pod
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - source_labels: [__meta_kubernetes_namespace]
          regex: default
          action: keep
        - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_instance]
          regex: predator-analytics
          action: keep
        - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_name]
          regex: alertmanager
          action: keep
        - source_labels: [__meta_kubernetes_pod_container_port_number]
          regex: "9093"
          action: keep
  recording_rules.yml: |
    {}
  rules: |
    {}
---
# Source: predator-analytics/charts/redis/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: predator-analytics-redis-configuration
  namespace: "default"
  labels:
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.0.12
    helm.sh/chart: redis-17.17.1
data:
  redis.conf: |-
    # User-supplied common configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
    # End of common configuration
  master.conf: |-
    dir /data
    # User-supplied master configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of master configuration
  replica.conf: |-
    dir /data
    # User-supplied replica configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of replica configuration
---
# Source: predator-analytics/charts/redis/templates/health-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: predator-analytics-redis-health
  namespace: "default"
  labels:
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.0.12
    helm.sh/chart: redis-17.17.1
data:
  ping_readiness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ] && [ "$responseFirstWord" != "MASTERDOWN" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
---
# Source: predator-analytics/charts/redis/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: predator-analytics-redis-scripts
  namespace: "default"
  labels:
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.0.12
    helm.sh/chart: redis-17.17.1
data:
  start-master.sh: |
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    if [[ -f /opt/bitnami/redis/mounted-etc/master.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
    fi
    if [[ -f /opt/bitnami/redis/mounted-etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--requirepass" "${REDIS_PASSWORD}")
    ARGS+=("--masterauth" "${REDIS_PASSWORD}")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
    exec redis-server "${ARGS[@]}"
  start-replica.sh: |
    #!/bin/bash

    get_port() {
        hostname="$1"
        type="$2"

        port_var=$(echo "${hostname^^}_SERVICE_PORT_$type" | sed "s/-/_/g")
        port=${!port_var}

        if [ -z "$port" ]; then
            case $type in
                "SENTINEL")
                    echo 26379
                    ;;
                "REDIS")
                    echo 6379
                    ;;
            esac
        else
            echo $port
        fi
    }

    get_full_hostname() {
        hostname="$1"
        full_hostname="${hostname}.${HEADLESS_SERVICE}"
        echo "${full_hostname}"
    }

    REDISPORT=$(get_port "$HOSTNAME" "REDIS")
    HEADLESS_SERVICE="predator-analytics-redis-headless.default.svc.cluster.local"

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    if [[ -f /opt/bitnami/redis/mounted-etc/replica.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/replica.conf /opt/bitnami/redis/etc/replica.conf
    fi
    if [[ -f /opt/bitnami/redis/mounted-etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi

    echo "" >> /opt/bitnami/redis/etc/replica.conf
    echo "replica-announce-port $REDISPORT" >> /opt/bitnami/redis/etc/replica.conf
    echo "replica-announce-ip $(get_full_hostname "$HOSTNAME")" >> /opt/bitnami/redis/etc/replica.conf
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--replicaof" "${REDIS_MASTER_HOST}" "${REDIS_MASTER_PORT_NUMBER}")
    ARGS+=("--requirepass" "${REDIS_PASSWORD}")
    ARGS+=("--masterauth" "${REDIS_MASTER_PASSWORD}")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/replica.conf")
    exec redis-server "${ARGS[@]}"
---
# Source: predator-analytics/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: predator-analytics-predator-analytics-model-config
  labels:
    app.kubernetes.io/name: predator-analytics
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/managed-by: Helm
data:
  model_registry.yaml: |-
    competitionScenarios:
      enabled: true
      evaluationInterval: 5m
      models:
        coding:
        - codestral-2501
        - deepseek/deepseek-coder-v2
        - phind/phind-codellama-34b-v2
        premium:
        - gpt-5
        - microsoft/phi-4-reasoning
        - qwen/qwen2.5-72b-instruct
        speed:
        - microsoft/phi-3-mini-4k-instruct
        - mistral/ministral-3b
        - google/gemma-2-2b-it
    enabled: true
    freeModels:
      code: 10
      embed: 8
      gen: 4
      quick: 8
      reasoning: 12
      vision: 6
---
# Source: predator-analytics/charts/prometheus/templates/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: v2.46.0
    helm.sh/chart: prometheus-23.4.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: prometheus
  name: predator-analytics-prometheus-server
  namespace: default
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: "8Gi"
---
# Source: predator-analytics/charts/grafana/templates/clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    helm.sh/chart: grafana-6.61.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "10.1.5"
    app.kubernetes.io/managed-by: Helm
  name: predator-analytics-grafana-clusterrole
rules: []
---
# Source: predator-analytics/charts/prometheus/charts/kube-state-metrics/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    helm.sh/chart: kube-state-metrics-5.10.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "2.9.2"
  name: predator-analytics-kube-state-metrics
rules:
  - apiGroups: ["certificates.k8s.io"]
    resources:
      - certificatesigningrequests
    verbs: ["list", "watch"]
  - apiGroups: [""]
    resources:
      - configmaps
    verbs: ["list", "watch"]
  - apiGroups: ["batch"]
    resources:
      - cronjobs
    verbs: ["list", "watch"]
  - apiGroups: ["extensions", "apps"]
    resources:
      - daemonsets
    verbs: ["list", "watch"]
  - apiGroups: ["extensions", "apps"]
    resources:
      - deployments
    verbs: ["list", "watch"]
  - apiGroups: [""]
    resources:
      - endpoints
    verbs: ["list", "watch"]
  - apiGroups: ["autoscaling"]
    resources:
      - horizontalpodautoscalers
    verbs: ["list", "watch"]
  - apiGroups: ["extensions", "networking.k8s.io"]
    resources:
      - ingresses
    verbs: ["list", "watch"]
  - apiGroups: ["batch"]
    resources:
      - jobs
    verbs: ["list", "watch"]
  - apiGroups: ["coordination.k8s.io"]
    resources:
      - leases
    verbs: ["list", "watch"]
  - apiGroups: [""]
    resources:
      - limitranges
    verbs: ["list", "watch"]
  - apiGroups: ["admissionregistration.k8s.io"]
    resources:
      - mutatingwebhookconfigurations
    verbs: ["list", "watch"]
  - apiGroups: [""]
    resources:
      - namespaces
    verbs: ["list", "watch"]
  - apiGroups: ["networking.k8s.io"]
    resources:
      - networkpolicies
    verbs: ["list", "watch"]
  - apiGroups: [""]
    resources:
      - nodes
    verbs: ["list", "watch"]
  - apiGroups: [""]
    resources:
      - persistentvolumeclaims
    verbs: ["list", "watch"]
  - apiGroups: [""]
    resources:
      - persistentvolumes
    verbs: ["list", "watch"]
  - apiGroups: ["policy"]
    resources:
      - poddisruptionbudgets
    verbs: ["list", "watch"]
  - apiGroups: [""]
    resources:
      - pods
    verbs: ["list", "watch"]
  - apiGroups: ["extensions", "apps"]
    resources:
      - replicasets
    verbs: ["list", "watch"]
  - apiGroups: [""]
    resources:
      - replicationcontrollers
    verbs: ["list", "watch"]
  - apiGroups: [""]
    resources:
      - resourcequotas
    verbs: ["list", "watch"]
  - apiGroups: [""]
    resources:
      - secrets
    verbs: ["list", "watch"]
  - apiGroups: [""]
    resources:
      - services
    verbs: ["list", "watch"]
  - apiGroups: ["apps"]
    resources:
      - statefulsets
    verbs: ["list", "watch"]
  - apiGroups: ["storage.k8s.io"]
    resources:
      - storageclasses
    verbs: ["list", "watch"]
  - apiGroups: ["admissionregistration.k8s.io"]
    resources:
      - validatingwebhookconfigurations
    verbs: ["list", "watch"]
  - apiGroups: ["storage.k8s.io"]
    resources:
      - volumeattachments
    verbs: ["list", "watch"]
---
# Source: predator-analytics/charts/prometheus/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: v2.46.0
    helm.sh/chart: prometheus-23.4.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: prometheus
  name: predator-analytics-prometheus-server
rules:
  - apiGroups:
      - ""
    resources:
      - nodes
      - nodes/proxy
      - nodes/metrics
      - services
      - endpoints
      - pods
      - ingresses
      - configmaps
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "extensions"
      - "networking.k8s.io"
    resources:
      - ingresses/status
      - ingresses
    verbs:
      - get
      - list
      - watch
  - nonResourceURLs:
      - "/metrics"
    verbs:
      - get
---
# Source: predator-analytics/charts/grafana/templates/clusterrolebinding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: predator-analytics-grafana-clusterrolebinding
  labels:
    helm.sh/chart: grafana-6.61.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "10.1.5"
    app.kubernetes.io/managed-by: Helm
subjects:
  - kind: ServiceAccount
    name: predator-analytics-grafana
    namespace: default
roleRef:
  kind: ClusterRole
  name: predator-analytics-grafana-clusterrole
  apiGroup: rbac.authorization.k8s.io
---
# Source: predator-analytics/charts/prometheus/charts/kube-state-metrics/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    helm.sh/chart: kube-state-metrics-5.10.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "2.9.2"
  name: predator-analytics-kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: predator-analytics-kube-state-metrics
subjects:
  - kind: ServiceAccount
    name: predator-analytics-kube-state-metrics
    namespace: default
---
# Source: predator-analytics/charts/prometheus/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: v2.46.0
    helm.sh/chart: prometheus-23.4.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: prometheus
  name: predator-analytics-prometheus-server
subjects:
  - kind: ServiceAccount
    name: predator-analytics-prometheus-server
    namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: predator-analytics-prometheus-server
---
# Source: predator-analytics/charts/grafana/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: predator-analytics-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-6.61.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "10.1.5"
    app.kubernetes.io/managed-by: Helm
rules: []
---
# Source: predator-analytics/charts/grafana/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: predator-analytics-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-6.61.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "10.1.5"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: predator-analytics-grafana
subjects:
  - kind: ServiceAccount
    name: predator-analytics-grafana
    namespace: default
---
# Source: predator-analytics/charts/grafana/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: predator-analytics-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-6.61.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "10.1.5"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: service
      port: 80
      protocol: TCP
      targetPort: 3000
  selector:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: predator-analytics
---
# Source: predator-analytics/charts/opensearch/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: predator-opensearch
  labels:
    helm.sh/chart: opensearch-2.35.1
    app.kubernetes.io/name: opensearch
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "2.19.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: predator-opensearch-master
  annotations: {}
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: opensearch
    app.kubernetes.io/instance: predator-analytics
  ports:
    - name: http
      protocol: TCP
      port: 9200
    - name: transport
      protocol: TCP
      port: 9300
    - name: metrics
      protocol: TCP
      port: 9600
---
# Source: predator-analytics/charts/opensearch/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: predator-opensearch-headless
  labels:
    helm.sh/chart: opensearch-2.35.1
    app.kubernetes.io/name: opensearch
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "2.19.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: predator-opensearch-master
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  clusterIP: None # This is needed for statefulset hostnames like opensearch-0 to resolve
  # Create endpoints also if the related pod isn't ready
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/name: opensearch
    app.kubernetes.io/instance: predator-analytics
  ports:
    - name: http
      port: 9200
    - name: transport
      port: 9300
    - name: metrics
      port: 9600
---
# Source: predator-analytics/charts/postgresql/templates/primary/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: predator-analytics-postgresql-hl
  namespace: "default"
  labels:
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 15.4.0
    helm.sh/chart: postgresql-12.12.10
    app.kubernetes.io/component: primary
  annotations:
    # Use this annotation in addition to the actual publishNotReadyAddresses
    # field below because the annotation will stop being respected soon but the
    # field is broken in some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  # We want all pods in the StatefulSet to have their addresses published for
  # the sake of the other Postgresql pods even before they're ready, since they
  # have to be able to talk to each other in order to become ready.
  publishNotReadyAddresses: true
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/component: primary
---
# Source: predator-analytics/charts/postgresql/templates/primary/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: predator-analytics-postgresql
  namespace: "default"
  labels:
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 15.4.0
    helm.sh/chart: postgresql-12.12.10
    app.kubernetes.io/component: primary
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
      nodePort: null
  selector:
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/component: primary
---
# Source: predator-analytics/charts/prometheus/charts/alertmanager/templates/services.yaml
apiVersion: v1
kind: Service
metadata:
  name: predator-analytics-alertmanager
  labels:
    helm.sh/chart: alertmanager-0.33.1
    app.kubernetes.io/name: alertmanager
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "v0.25.0"
    app.kubernetes.io/managed-by: Helm
  namespace: default
spec:
  type: ClusterIP
  ports:
    - port: 9093
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: alertmanager
    app.kubernetes.io/instance: predator-analytics
---
# Source: predator-analytics/charts/prometheus/charts/alertmanager/templates/services.yaml
apiVersion: v1
kind: Service
metadata:
  name: predator-analytics-alertmanager-headless
  labels:
    helm.sh/chart: alertmanager-0.33.1
    app.kubernetes.io/name: alertmanager
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "v0.25.0"
    app.kubernetes.io/managed-by: Helm
  namespace: default
spec:
  clusterIP: None
  ports:
    - port: 9093
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: alertmanager
    app.kubernetes.io/instance: predator-analytics
---
# Source: predator-analytics/charts/prometheus/charts/kube-state-metrics/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: predator-analytics-kube-state-metrics
  namespace: default
  labels:
    helm.sh/chart: kube-state-metrics-5.10.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "2.9.2"
  annotations:
    prometheus.io/scrape: 'true'
spec:
  type: "ClusterIP"
  ports:
    - name: "http"
      protocol: TCP
      port: 8080
      targetPort: 8080
  selector:
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: predator-analytics
---
# Source: predator-analytics/charts/prometheus/charts/prometheus-node-exporter/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: predator-analytics-prometheus-node-exporter
  namespace: default
  labels:
    helm.sh/chart: prometheus-node-exporter-4.21.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "1.6.0"
  annotations:
    prometheus.io/scrape: "true"
spec:
  type: ClusterIP
  ports:
    - port: 9100
      targetPort: 9100
      protocol: TCP
      name: metrics
  selector:
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: predator-analytics
---
# Source: predator-analytics/charts/prometheus/charts/prometheus-pushgateway/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/probe: pushgateway
  labels:
    helm.sh/chart: prometheus-pushgateway-2.4.0
    app.kubernetes.io/name: prometheus-pushgateway
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "v1.6.0"
    app.kubernetes.io/managed-by: Helm
  name: predator-analytics-prometheus-pushgateway
  namespace: default
spec:
  type: ClusterIP
  ports:
    - port: 9091
      targetPort: 9091
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: prometheus-pushgateway
    app.kubernetes.io/instance: predator-analytics
---
# Source: predator-analytics/charts/prometheus/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: v2.46.0
    helm.sh/chart: prometheus-23.4.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: prometheus
  name: predator-analytics-prometheus-server
  namespace: default
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 9090
  selector:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: predator-analytics
  sessionAffinity: None
  type: "ClusterIP"
---
# Source: predator-analytics/charts/redis/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: predator-analytics-redis-headless
  namespace: "default"
  labels:
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.0.12
    helm.sh/chart: redis-17.17.1
  annotations:
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/name: redis
---
# Source: predator-analytics/charts/redis/templates/master/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: predator-analytics-redis-master
  namespace: "default"
  labels:
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.0.12
    helm.sh/chart: redis-17.17.1
    app.kubernetes.io/component: master
spec:
  type: ClusterIP
  internalTrafficPolicy: Cluster
  sessionAffinity: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
      nodePort: null
  selector:
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/name: redis
    app.kubernetes.io/component: master
---
# Source: predator-analytics/charts/redis/templates/replicas/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: predator-analytics-redis-replicas
  namespace: "default"
  labels:
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.0.12
    helm.sh/chart: redis-17.17.1
    app.kubernetes.io/component: replica
spec:
  type: ClusterIP
  internalTrafficPolicy: Cluster
  sessionAffinity: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
      nodePort: null
  selector:
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/name: redis
    app.kubernetes.io/component: replica
---
# Source: predator-analytics/charts/prometheus/charts/prometheus-node-exporter/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: predator-analytics-prometheus-node-exporter
  namespace: default
  labels:
    helm.sh/chart: prometheus-node-exporter-4.21.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "1.6.0"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/instance: predator-analytics
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        helm.sh/chart: prometheus-node-exporter-4.21.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: metrics
        app.kubernetes.io/part-of: prometheus-node-exporter
        app.kubernetes.io/name: prometheus-node-exporter
        app.kubernetes.io/instance: predator-analytics
        app.kubernetes.io/version: "1.6.0"
    spec:
      automountServiceAccountToken: false
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      serviceAccountName: predator-analytics-prometheus-node-exporter
      containers:
        - name: node-exporter
          image: quay.io/prometheus/node-exporter:quicktest-20251030
          imagePullPolicy: IfNotPresent
          args:
            - --path.procfs=/host/proc
            - --path.sysfs=/host/sys
            - --path.rootfs=/host/root
            - --path.udev.data=/host/root/run/udev/data
            - --web.listen-address=[$(HOST_IP)]:9100
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
          env:
            - name: HOST_IP
              value: 0.0.0.0
          ports:
            - name: metrics
              containerPort: 9100
              protocol: TCP
          livenessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              path: /
              port: 9100
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          readinessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              path: /
              port: 9100
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          volumeMounts:
            - name: proc
              mountPath: /host/proc
              readOnly: true
            - name: sys
              mountPath: /host/sys
              readOnly: true
            - name: root
              mountPath: /host/root
              mountPropagation: HostToContainer
              readOnly: true
      hostNetwork: true
      hostPID: true
      nodeSelector:
        kubernetes.io/os: linux
      tolerations:
        - effect: NoSchedule
          operator: Exists
      volumes:
        - name: proc
          hostPath:
            path: /proc
        - name: sys
          hostPath:
            path: /sys
        - name: root
          hostPath:
            path: /
---
# Source: predator-analytics/charts/grafana/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: predator-analytics-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-6.61.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "10.1.5"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: grafana
      app.kubernetes.io/instance: predator-analytics
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: grafana
        app.kubernetes.io/instance: predator-analytics
      annotations:
        checksum/config: a062cb18ae0508f36a70055f3674a40091de37f1bcd259b959b6b5fccd2a1d62
        checksum/dashboards-json-config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/sc-dashboard-provider-config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/secret: c1dba7cafe64cb5a8353e330cb2fbf6d767bc0afe69b81b41e1bfee761c6c98a
        kubectl.kubernetes.io/default-container: grafana
    spec:
      serviceAccountName: predator-analytics-grafana
      automountServiceAccountToken: true
      securityContext:
        fsGroup: 472
        runAsGroup: 472
        runAsNonRoot: true
        runAsUser: 472
      enableServiceLinks: true
      containers:
        - name: grafana
          image: "docker.io/grafana/grafana:quicktest-20251030"
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - name: config
              mountPath: "/etc/grafana/grafana.ini"
              subPath: grafana.ini
            - name: storage
              mountPath: "/var/lib/grafana"
          ports:
            - name: grafana
              containerPort: 3000
              protocol: TCP
            - name: gossip-tcp
              containerPort: 9094
              protocol: TCP
            - name: gossip-udp
              containerPort: 9094
              protocol: UDP
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: GF_SECURITY_ADMIN_USER
              valueFrom:
                secretKeyRef:
                  name: predator-analytics-grafana
                  key: admin-user
            - name: GF_SECURITY_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: predator-analytics-grafana
                  key: admin-password
            - name: GF_PATHS_DATA
              value: /var/lib/grafana/
            - name: GF_PATHS_LOGS
              value: /var/log/grafana
            - name: GF_PATHS_PLUGINS
              value: /var/lib/grafana/plugins
            - name: GF_PATHS_PROVISIONING
              value: /etc/grafana/provisioning
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
            initialDelaySeconds: 60
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /api/health
              port: 3000
      volumes:
        - name: config
          configMap:
            name: predator-analytics-grafana
        - name: storage
          emptyDir: {}
---
# Source: predator-analytics/charts/prometheus/charts/kube-state-metrics/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: predator-analytics-kube-state-metrics
  namespace: default
  labels:
    helm.sh/chart: kube-state-metrics-5.10.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "2.9.2"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/instance: predator-analytics
  replicas: 1
  template:
    metadata:
      labels:
        helm.sh/chart: kube-state-metrics-5.10.1
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: metrics
        app.kubernetes.io/part-of: kube-state-metrics
        app.kubernetes.io/name: kube-state-metrics
        app.kubernetes.io/instance: predator-analytics
        app.kubernetes.io/version: "2.9.2"
    spec:
      hostNetwork: false
      serviceAccountName: predator-analytics-kube-state-metrics
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
        seccompProfile:
          type: RuntimeDefault
      containers:
        - name: kube-state-metrics
          args:
            - --port=8080
            - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          imagePullPolicy: IfNotPresent
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:quicktest-20251030
          ports:
            - containerPort: 8080
              name: "http"
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8080
            initialDelaySeconds: 5
            timeoutSeconds: 5
          readinessProbe:
            httpGet:
              path: /
              port: 8080
            initialDelaySeconds: 5
            timeoutSeconds: 5
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
---
# Source: predator-analytics/charts/prometheus/charts/prometheus-pushgateway/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    helm.sh/chart: prometheus-pushgateway-2.4.0
    app.kubernetes.io/name: prometheus-pushgateway
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "v1.6.0"
    app.kubernetes.io/managed-by: Helm
  name: predator-analytics-prometheus-pushgateway
  namespace: default
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app.kubernetes.io/name: prometheus-pushgateway
      app.kubernetes.io/instance: predator-analytics
  template:
    metadata:
      labels:
        helm.sh/chart: prometheus-pushgateway-2.4.0
        app.kubernetes.io/name: prometheus-pushgateway
        app.kubernetes.io/instance: predator-analytics
        app.kubernetes.io/version: "v1.6.0"
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: predator-analytics-prometheus-pushgateway
      containers:
        - name: pushgateway
          image: "quay.io/prometheus/pushgateway:quicktest-20251030"
          imagePullPolicy: IfNotPresent
          ports:
            - name: metrics
              containerPort: 9091
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /-/healthy
              port: 9091
            initialDelaySeconds: 10
            timeoutSeconds: 10
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 9091
            initialDelaySeconds: 10
            timeoutSeconds: 10
          volumeMounts:
            - name: storage-volume
              mountPath: "/data"
              subPath: ""
      securityContext:
        fsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      volumes:
        - name: storage-volume
          emptyDir: {}
---
# Source: predator-analytics/charts/prometheus/templates/deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: v2.46.0
    helm.sh/chart: prometheus-23.4.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: prometheus
  name: predator-analytics-prometheus-server
  namespace: default
spec:
  selector:
    matchLabels:
      app.kubernetes.io/component: server
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/instance: predator-analytics
  replicas: 1
  revisionHistoryLimit: 10
  strategy:
    type: Recreate
    rollingUpdate: null
  template:
    metadata:
      labels:
        app.kubernetes.io/component: server
        app.kubernetes.io/name: prometheus
        app.kubernetes.io/instance: predator-analytics
        app.kubernetes.io/version: v2.46.0
        helm.sh/chart: prometheus-23.4.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/part-of: prometheus
    spec:
      enableServiceLinks: true
      serviceAccountName: predator-analytics-prometheus-server
      containers:
        - name: prometheus-server-configmap-reload
          image: "quay.io/prometheus-operator/prometheus-config-reloader:quicktest-20251030"
          imagePullPolicy: "IfNotPresent"
          args:
            - --watched-dir=/etc/config
            - --reload-url=http://127.0.0.1:9090/-/reload
          resources: {}
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
              readOnly: true
        - name: prometheus-server
          image: "quay.io/prometheus/prometheus:quicktest-20251030"
          imagePullPolicy: "IfNotPresent"
          args:
            - --storage.tsdb.retention.time=15d
            - --config.file=/etc/config/prometheus.yml
            - --storage.tsdb.path=/data
            - --web.console.libraries=/etc/prometheus/console_libraries
            - --web.console.templates=/etc/prometheus/consoles
            - --web.enable-lifecycle
          ports:
            - containerPort: 9090
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            timeoutSeconds: 4
            failureThreshold: 3
            successThreshold: 1
          livenessProbe:
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 15
            timeoutSeconds: 10
            failureThreshold: 3
            successThreshold: 1
          resources: {}
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
            - name: storage-volume
              mountPath: /data
              subPath: ""
      dnsPolicy: ClusterFirst
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      terminationGracePeriodSeconds: 300
      volumes:
        - name: config-volume
          configMap:
            name: predator-analytics-prometheus-server
        - name: storage-volume
          persistentVolumeClaim:
            claimName: predator-analytics-prometheus-server
---
# Source: predator-analytics/templates/agents-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: predator-analytics-predator-analytics-agents-reasoning
  labels:
    app.kubernetes.io/name: predator-analytics
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: agents-reasoning
spec:
  replicas: 5
  selector:
    matchLabels:
      app.kubernetes.io/name: predator-analytics
      app.kubernetes.io/instance: predator-analytics
      app.kubernetes.io/component: agents-reasoning
  template:
    metadata:
      labels:
        app.kubernetes.io/name: predator-analytics
        app.kubernetes.io/instance: predator-analytics
        app.kubernetes.io/component: agents-reasoning
    spec:
      serviceAccountName: predator-analytics-sa
      containers:
        - name: reasoning-agent
          image: "docker.io/predator-analytics/backend:auto-1761852878"
          command: ["python", "-m", "agents.reasoning_agent"]
          env:
            - name: AGENT_TYPE
              value: "reasoning"
            - name: PRIMARY_MODEL
              value: "meta/meta-llama-3.1-70b-instruct"
            - name: FALLBACK_MODELS
              value: "microsoft/phi-4-reasoning,qwen/qwen2.5-72b-instruct"
            - name: FEEDBACK_LEVELS
              value: "4"
            - name: FEEDBACK_THRESHOLDS
              value: "0.9,0.75,0.6,0.4"
            - name: ADAPTIVE_ROUTING
              value: "true"
            - name: REDIS_URL
              value: "redis://:$(REDIS_PASSWORD)@predator-analytics-predator-analytics-redis-master:6379/0"
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: predator-analytics-predator-analytics-redis
                  key: redis-password
          resources: null
---
# Source: predator-analytics/templates/agents-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: predator-analytics-predator-analytics-agents-code
  labels:
    app.kubernetes.io/name: predator-analytics
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: agents-code
spec:
  replicas: 3
  selector:
    matchLabels:
      app.kubernetes.io/name: predator-analytics
      app.kubernetes.io/instance: predator-analytics
      app.kubernetes.io/component: agents-code
  template:
    metadata:
      labels:
        app.kubernetes.io/name: predator-analytics
        app.kubernetes.io/instance: predator-analytics
        app.kubernetes.io/component: agents-code
    spec:
      serviceAccountName: predator-analytics-sa
      containers:
        - name: code-agent
          image: "docker.io/predator-analytics/backend:auto-1761852878"
          command: ["python", "-m", "agents.code_agent"]
          env:
            - name: AGENT_TYPE
              value: "code"
            - name: PRIMARY_MODEL
              value: "codestral-2501"
            - name: FALLBACK_MODELS
              value: "deepseek/deepseek-coder-v2,qwen/qwen2.5-coder-7b-instruct"
            - name: FEEDBACK_LEVELS
              value: "4"
            - name: FEEDBACK_THRESHOLDS
              value: "0.9,0.75,0.6,0.4"
            - name: REDIS_URL
              value: "redis://:$(REDIS_PASSWORD)@predator-analytics-predator-analytics-redis-master:6379/0"
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: predator-analytics-predator-analytics-redis
                  key: redis-password
          resources: null
---
# Source: predator-analytics/templates/agents-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: predator-analytics-predator-analytics-agents-quick
  labels:
    app.kubernetes.io/name: predator-analytics
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: agents-quick
spec:
  replicas: 8
  selector:
    matchLabels:
      app.kubernetes.io/name: predator-analytics
      app.kubernetes.io/instance: predator-analytics
      app.kubernetes.io/component: agents-quick
  template:
    metadata:
      labels:
        app.kubernetes.io/name: predator-analytics
        app.kubernetes.io/instance: predator-analytics
        app.kubernetes.io/component: agents-quick
    spec:
      serviceAccountName: predator-analytics-sa
      containers:
        - name: quick-agent
          image: "docker.io/predator-analytics/backend:auto-1761852878"
          command: ["python", "-m", "agents.quick_agent"]
          env:
            - name: AGENT_TYPE
              value: "quick"
            - name: PRIMARY_MODEL
              value: "microsoft/phi-3-mini-4k-instruct"
            - name: FALLBACK_MODELS
              value: "mistral/ministral-3b,google/gemma-2-2b-it"
            - name: FEEDBACK_LEVELS
              value: "4"
            - name: FEEDBACK_THRESHOLDS
              value: "0.9,0.75,0.6,0.4"
            - name: REDIS_URL
              value: "redis://:$(REDIS_PASSWORD)@predator-analytics-predator-analytics-redis-master:6379/0"
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: predator-analytics-predator-analytics-redis
                  key: redis-password
          resources: null
---
# Source: predator-analytics/templates/backend-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: predator-analytics-predator-analytics-backend
  labels:
    app.kubernetes.io/name: predator-analytics
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: backend
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: predator-analytics
      app.kubernetes.io/instance: predator-analytics
      app.kubernetes.io/component: backend
  template:
    metadata:
      annotations:
        checksum/config: f4df442ee0be18ff51202382a9aef30295b6e820b3fc4916dfce3d16711f894c
      labels:
        app.kubernetes.io/name: predator-analytics
        app.kubernetes.io/instance: predator-analytics
        app.kubernetes.io/component: backend
    spec:
      serviceAccountName: predator-analytics-sa
      securityContext: null
      containers:
        - name: backend
          securityContext: null
          image: "docker.io/predator-analytics/backend:auto-1761852878"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
          env:
            - name: DATABASE_URL
              value: "postgresql://predator:$(POSTGRES_PASSWORD)@predator-analytics-predator-analytics-postgresql:5432/predator_analytics"
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: predator-analytics-predator-analytics-postgresql
                  key: postgres-password
            - name: REDIS_URL
              value: "redis://:$(REDIS_PASSWORD)@predator-analytics-predator-analytics-redis-master:6379/0"
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: predator-analytics-predator-analytics-redis
                  key: redis-password
            - name: ADAPTIVE_ROUTING
              value: "true"
            - name: AI_FEEDBACK_LEVELS
              value: "4"
            - name: COST_OPTIMIZATION
              value: "true"
            - name: MODELS_ENABLED
              value: "58_free_models"
            - name: OPENSEARCH_URL
              value: "http://predator-analytics-predator-analytics-opensearch:9200"
            - name: FREE_MODELS_COUNT
              value: "58"
            - name: FEEDBACK_LEVELS
              value: "4"
            - name: ADAPTIVE_ROUTING_ENABLED
              value: "true"
            - name: PERFORMANCE_TRACKING_ENABLED
              value: "true"
          livenessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 60
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 30
            periodSeconds: 5
          resources:
            limits:
              cpu: 1000m
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 1Gi
          volumeMounts:
            - name: model-config
              mountPath: /app/model_registry.yaml
              subPath: model_registry.yaml
              readOnly: true
      volumes:
        - name: model-config
          configMap:
            name: predator-analytics-predator-analytics-model-config
---
# Source: predator-analytics/templates/frontend-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: predator-analytics-predator-analytics-frontend
  labels:
    app.kubernetes.io/name: predator-analytics
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: frontend
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: predator-analytics
      app.kubernetes.io/instance: predator-analytics
      app.kubernetes.io/component: frontend
  template:
    metadata:
      annotations:
        checksum/config: f4df442ee0be18ff51202382a9aef30295b6e820b3fc4916dfce3d16711f894c
      labels:
        app.kubernetes.io/name: predator-analytics
        app.kubernetes.io/instance: predator-analytics
        app.kubernetes.io/component: frontend
    spec:
      serviceAccountName: predator-analytics-sa
      securityContext: null
      containers:
        - name: frontend
          securityContext: null
          image: "docker.io/predator-analytics/frontend:auto-1761852878"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 3000
              protocol: TCP
          env:
            - name: REACT_APP_API_URL
              value: "http://predator-analytics-predator-analytics-backend:8000"
            - name: REACT_APP_VOICE_ENABLED
              value: "true"
            - name: REACT_APP_SPEECH_LANG
              value: "uk-UA"
            - name: REACT_APP_MODELS_COUNT
              value: "58"
            - name: REACT_APP_AI_FEEDBACK_LEVELS
              value: "4"
          livenessProbe:
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 5
            periodSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 250m
              memory: 256Mi
---
# Source: predator-analytics/charts/opensearch/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: predator-opensearch-master
  labels:
    helm.sh/chart: opensearch-2.35.1
    app.kubernetes.io/name: opensearch
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "2.19.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: predator-opensearch-master
  annotations:
    majorVersion: "2"
spec:
  serviceName: predator-opensearch-headless
  selector:
    matchLabels:
      app.kubernetes.io/name: opensearch
      app.kubernetes.io/instance: predator-analytics
  replicas: 3
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  volumeClaimTemplates:
    - metadata:
        name: predator-opensearch-master
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "200Gi"
  template:
    metadata:
      name: "predator-opensearch-master"
      labels:
        helm.sh/chart: opensearch-2.35.1
        app.kubernetes.io/name: opensearch
        app.kubernetes.io/instance: predator-analytics
        app.kubernetes.io/version: "2.19.3"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: predator-opensearch-master
      annotations:
        configchecksum: 18d1711d7f9b6a5c3971a044eed7896e238527e9a06a73ff07a35411227d713
    spec:
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      automountServiceAccountToken: false
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 1
              podAffinityTerm:
                topologyKey: kubernetes.io/hostname
                labelSelector:
                  matchExpressions:
                    - key: app.kubernetes.io/instance
                      operator: In
                      values:
                        - predator-analytics
                    - key: app.kubernetes.io/name
                      operator: In
                      values:
                        - opensearch
      terminationGracePeriodSeconds: 120
      volumes:
        - name: config
          configMap:
            name: predator-opensearch-master-config
        - emptyDir: {}
          name: config-emptydir
      enableServiceLinks: true
      initContainers:
        - name: fsgroup-volume
          image: "busybox:quicktest-20251030"
          imagePullPolicy: "IfNotPresent"
          command: ['sh', '-c']
          args:
            - 'chown -R 1000:1000 /usr/share/opensearch/data'
          securityContext:
            runAsUser: 0
          resources: {}
          volumeMounts:
            - name: "predator-opensearch-master"
              mountPath: /usr/share/opensearch/data
        - name: configfile
          image: "opensearchproject/opensearch:quicktest-20251030"
          imagePullPolicy: "IfNotPresent"
          command:
            - sh
            - -c
            - |
              #!/usr/bin/env bash
              cp -r /tmp/configfolder/*  /tmp/config/
          securityContext:
            capabilities:
              drop:
                - ALL
            runAsNonRoot: true
            runAsUser: 1000
          resources: {}
          volumeMounts:
            - mountPath: /tmp/config/
              name: config-emptydir
            - name: config
              mountPath: /tmp/configfolder/opensearch.yml
              subPath: opensearch.yml
      containers:
        - name: "opensearch"
          securityContext:
            capabilities:
              drop:
                - ALL
            runAsNonRoot: true
            runAsUser: 1000
          image: "opensearchproject/opensearch:quicktest-20251030"
          imagePullPolicy: "IfNotPresent"
          readinessProbe:
            failureThreshold: 3
            periodSeconds: 5
            tcpSocket:
              port: 9200
            timeoutSeconds: 3
          startupProbe:
            failureThreshold: 30
            initialDelaySeconds: 5
            periodSeconds: 10
            tcpSocket:
              port: 9200
            timeoutSeconds: 3
          ports:
            - name: http
              containerPort: 9200
            - name: transport
              containerPort: 9300
            - name: metrics
              containerPort: 9600
          resources:
            limits:
              cpu: 2000m
              memory: 4Gi
            requests:
              cpu: 1000m
              memory: 2Gi
          env:
            - name: node.name
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: cluster.initial_master_nodes
              value: "predator-opensearch-master-0,predator-opensearch-master-1,predator-opensearch-master-2,"
            - name: discovery.seed_hosts
              value: "predator-opensearch-headless"
            - name: cluster.name
              value: "predator-opensearch"
            - name: network.host
              value: "0.0.0.0"
            - name: OPENSEARCH_JAVA_OPTS
              value: "-Xmx512M -Xms512M"
            - name: node.roles
              value: "master,ingest,data,remote_cluster_client,"
          volumeMounts:
            - name: "predator-opensearch-master"
              mountPath: /usr/share/opensearch/data
            - name: config-emptydir
              mountPath: /usr/share/opensearch/config/opensearch.yml
              subPath: opensearch.yml
---
# Source: predator-analytics/charts/postgresql/templates/primary/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: predator-analytics-postgresql
  namespace: "default"
  labels:
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 15.4.0
    helm.sh/chart: postgresql-12.12.10
    app.kubernetes.io/component: primary
spec:
  replicas: 1
  serviceName: predator-analytics-postgresql-hl
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: predator-analytics
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/component: primary
  template:
    metadata:
      name: predator-analytics-postgresql
      labels:
        app.kubernetes.io/instance: predator-analytics
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: postgresql
        app.kubernetes.io/version: 15.4.0
        helm.sh/chart: postgresql-12.12.10
        app.kubernetes.io/component: primary
    spec:
      serviceAccountName: default
      affinity:
        podAffinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: predator-analytics
                    app.kubernetes.io/name: postgresql
                    app.kubernetes.io/component: primary
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
      securityContext:
        fsGroup: 1001
      hostNetwork: false
      hostIPC: false
      containers:
        - name: postgresql
          image: docker.io/bitnami/postgresql:quicktest-20251030
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            runAsGroup: 0
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: "/bitnami/postgresql"
            - name: PGDATA
              value: "/bitnami/postgresql/data"
            # Authentication
            - name: POSTGRES_USER
              value: "predator"
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: predator-analytics-postgresql
                  key: password
            - name: POSTGRES_POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: predator-analytics-postgresql
                  key: postgres-password
            - name: POSTGRES_DATABASE
              value: "predator_analytics"
            # Replication
            # Initdb
            # Standby
            # LDAP
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
            # TLS
            - name: POSTGRESQL_ENABLE_TLS
              value: "no"
            # Audit
            - name: POSTGRESQL_LOG_HOSTNAME
              value: "false"
            - name: POSTGRESQL_LOG_CONNECTIONS
              value: "false"
            - name: POSTGRESQL_LOG_DISCONNECTIONS
              value: "false"
            - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
              value: "off"
            # Others
            - name: POSTGRESQL_CLIENT_MIN_MESSAGES
              value: "error"
            - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
              value: "pgaudit"
          ports:
            - name: tcp-postgresql
              containerPort: 5432
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "predator" -d "dbname=predator_analytics" -h 127.0.0.1 -p 5432
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                - |
                  exec pg_isready -U "predator" -d "dbname=predator_analytics" -h 127.0.0.1 -p 5432
                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
          resources:
            limits:
              cpu: 2000m
              memory: 4Gi
            requests:
              cpu: 1000m
              memory: 2Gi
          volumeMounts:
            - name: dshm
              mountPath: /dev/shm
            - name: data
              mountPath: /bitnami/postgresql
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
  volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "100Gi"
---
# Source: predator-analytics/charts/prometheus/charts/alertmanager/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: predator-analytics-alertmanager
  labels:
    helm.sh/chart: alertmanager-0.33.1
    app.kubernetes.io/name: alertmanager
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "v0.25.0"
    app.kubernetes.io/managed-by: Helm
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: alertmanager
      app.kubernetes.io/instance: predator-analytics
  serviceName: predator-analytics-alertmanager-headless
  template:
    metadata:
      labels:
        app.kubernetes.io/name: alertmanager
        app.kubernetes.io/instance: predator-analytics
      annotations:
        checksum/config: 931922d4ebb38bf8f9e7a4ca6b9893f8b11ce588e62e961e3903c05e4cf0bdc8
    spec:
      automountServiceAccountToken: true
      serviceAccountName: predator-analytics-alertmanager
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      containers:
        - name: alertmanager
          securityContext:
            runAsGroup: 65534
            runAsNonRoot: true
            runAsUser: 65534
          image: "quay.io/prometheus/alertmanager:quicktest-20251030"
          imagePullPolicy: IfNotPresent
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
          args:
            - --storage.path=/alertmanager
            - --config.file=/etc/alertmanager/alertmanager.yml
          ports:
            - name: http
              containerPort: 9093
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources: {}
          volumeMounts:
            - name: config
              mountPath: /etc/alertmanager
            - name: storage
              mountPath: /alertmanager
      volumes:
        - name: config
          configMap:
            name: predator-analytics-alertmanager
  volumeClaimTemplates:
    - metadata:
        name: storage
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 2Gi
---
# Source: predator-analytics/charts/redis/templates/master/application.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: predator-analytics-redis-master
  namespace: "default"
  labels:
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.0.12
    helm.sh/chart: redis-17.17.1
    app.kubernetes.io/component: master
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: predator-analytics
      app.kubernetes.io/name: redis
      app.kubernetes.io/component: master
  serviceName: predator-analytics-redis-headless
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: predator-analytics
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: redis
        app.kubernetes.io/version: 7.0.12
        helm.sh/chart: redis-17.17.1
        app.kubernetes.io/component: master
      annotations:
        checksum/configmap: 86bcc953bb473748a3d3dc60b7c11f34e60c93519234d4c37f42e22ada559d47
        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
        checksum/scripts: 262404da077035f21dca4eb833119632e4ebfab3baa72148c252c6f52829c0af
        checksum/secret: 274bc98c8c7f26407d499724c7795ff65872fcc6ef1bb31b65ae257fd260f519
    spec:
      securityContext:
        fsGroup: 1001
      serviceAccountName: predator-analytics-redis
      automountServiceAccountToken: true
      affinity:
        podAffinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: predator-analytics
                    app.kubernetes.io/name: redis
                    app.kubernetes.io/component: master
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
      enableServiceLinks: true
      terminationGracePeriodSeconds: 30
      containers:
        - name: redis
          image: docker.io/bitnami/redis:quicktest-20251030
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            runAsGroup: 0
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - /bin/bash
          args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-master.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: master
            - name: ALLOW_EMPTY_PASSWORD
              value: "no"
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: predator-analytics-redis
                  key: redis-password
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          ports:
            - name: redis
              containerPort: 6379
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            # One second longer than command timeout should prevent generation of zombie processes.
            timeoutSeconds: 6
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local.sh 5
          readinessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 2
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local.sh 1
          resources:
            limits:
              cpu: 1000m
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 1Gi
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
            - name: config
              mountPath: /opt/bitnami/redis/mounted-etc
            - name: redis-tmp-conf
              mountPath: /opt/bitnami/redis/etc/
            - name: tmp
              mountPath: /tmp
      volumes:
        - name: start-scripts
          configMap:
            name: predator-analytics-redis-scripts
            defaultMode: 0755
        - name: health
          configMap:
            name: predator-analytics-redis-health
            defaultMode: 0755
        - name: config
          configMap:
            name: predator-analytics-redis-configuration
        - name: redis-tmp-conf
          emptyDir: {}
        - name: tmp
          emptyDir: {}
  volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: redis-data
        labels:
          app.kubernetes.io/instance: predator-analytics
          app.kubernetes.io/name: redis
          app.kubernetes.io/component: master
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "20Gi"
---
# Source: predator-analytics/charts/redis/templates/replicas/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: predator-analytics-redis-replicas
  namespace: "default"
  labels:
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.0.12
    helm.sh/chart: redis-17.17.1
    app.kubernetes.io/component: replica
spec:
  replicas: 3
  selector:
    matchLabels:
      app.kubernetes.io/instance: predator-analytics
      app.kubernetes.io/name: redis
      app.kubernetes.io/component: replica
  serviceName: predator-analytics-redis-headless
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: predator-analytics
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: redis
        app.kubernetes.io/version: 7.0.12
        helm.sh/chart: redis-17.17.1
        app.kubernetes.io/component: replica
      annotations:
        checksum/configmap: 86bcc953bb473748a3d3dc60b7c11f34e60c93519234d4c37f42e22ada559d47
        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
        checksum/scripts: 262404da077035f21dca4eb833119632e4ebfab3baa72148c252c6f52829c0af
        checksum/secret: e09cb02864926b0ac20bcc05d68a7fcb7ce1f7bfef85a8badee0aebe61d949d8
    spec:
      securityContext:
        fsGroup: 1001
      serviceAccountName: predator-analytics-redis
      automountServiceAccountToken: true
      affinity:
        podAffinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: predator-analytics
                    app.kubernetes.io/name: redis
                    app.kubernetes.io/component: replica
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
      enableServiceLinks: true
      terminationGracePeriodSeconds: 30
      containers:
        - name: redis
          image: docker.io/bitnami/redis:quicktest-20251030
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            runAsGroup: 0
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - /bin/bash
          args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-replica.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: replica
            - name: REDIS_MASTER_HOST
              value: predator-analytics-redis-master-0.predator-analytics-redis-headless.default.svc.cluster.local
            - name: REDIS_MASTER_PORT_NUMBER
              value: "6379"
            - name: ALLOW_EMPTY_PASSWORD
              value: "no"
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: predator-analytics-redis
                  key: redis-password
            - name: REDIS_MASTER_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: predator-analytics-redis
                  key: redis-password
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          ports:
            - name: redis
              containerPort: 6379
          startupProbe:
            failureThreshold: 22
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: redis
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 6
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local_and_master.sh 5
          readinessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 2
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local_and_master.sh 1
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
            - name: config
              mountPath: /opt/bitnami/redis/mounted-etc
            - name: redis-tmp-conf
              mountPath: /opt/bitnami/redis/etc
      volumes:
        - name: start-scripts
          configMap:
            name: predator-analytics-redis-scripts
            defaultMode: 0755
        - name: health
          configMap:
            name: predator-analytics-redis-health
            defaultMode: 0755
        - name: config
          configMap:
            name: predator-analytics-redis-configuration
        - name: redis-tmp-conf
          emptyDir: {}
  volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: redis-data
        labels:
          app.kubernetes.io/instance: predator-analytics
          app.kubernetes.io/name: redis
          app.kubernetes.io/component: replica
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: predator-analytics/charts/grafana/templates/tests/test-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    helm.sh/chart: grafana-6.61.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "10.1.5"
    app.kubernetes.io/managed-by: Helm
  name: predator-analytics-grafana-test
  namespace: default
  annotations:
    "helm.sh/hook": test-success
    "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
---
# Source: predator-analytics/charts/grafana/templates/tests/test-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: predator-analytics-grafana-test
  namespace: default
  annotations:
    "helm.sh/hook": test-success
    "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
  labels:
    helm.sh/chart: grafana-6.61.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "10.1.5"
    app.kubernetes.io/managed-by: Helm
data:
  run.sh: |-
    @test "Test Health" {
      url="http://predator-analytics-grafana/api/health"

      code=$(wget --server-response --spider --timeout 90 --tries 10 ${url} 2>&1 | awk '/^  HTTP/{print $2}')
      [ "$code" == "200" ]
    }
---
# Source: predator-analytics/charts/grafana/templates/tests/test.yaml
apiVersion: v1
kind: Pod
metadata:
  name: predator-analytics-grafana-test
  labels:
    helm.sh/chart: grafana-6.61.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: predator-analytics
    app.kubernetes.io/version: "10.1.5"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test-success
    "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
  namespace: default
spec:
  serviceAccountName: predator-analytics-grafana-test
  containers:
    - name: predator-analytics-test
      image: "docker.io/bats/bats:quicktest-20251030"
      imagePullPolicy: "IfNotPresent"
      command: ["/opt/bats/bin/bats", "-t", "/tests/run.sh"]
      volumeMounts:
        - mountPath: /tests
          name: tests
          readOnly: true
  volumes:
    - name: tests
      configMap:
        name: predator-analytics-grafana-test
  restartPolicy: Never
